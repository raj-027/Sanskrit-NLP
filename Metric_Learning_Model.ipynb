{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/raj-027/Sanskrit-NLP/blob/main/Metric_Learning_Model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i_XJFRvNoyP3"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import json\n",
        "import unicodedata\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "import torch.nn as nn\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gP-fH9qmO_GH"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0x5e0xDVqX6_"
      },
      "outputs": [],
      "source": [
        "\n",
        "def load_phoneme_vectors(csv_path):\n",
        "    df = pd.read_csv(csv_path)\n",
        "\n",
        "    phoneme_col = df.columns[0]\n",
        "    feature_cols = df.columns[1:]\n",
        "\n",
        "    phoneme2vec = {}\n",
        "    for _, row in df.iterrows():\n",
        "        phoneme = row[phoneme_col]\n",
        "        vector = row[feature_cols].astype(int).values\n",
        "        phoneme2vec[phoneme] = vector\n",
        "\n",
        "    feature_dim = len(feature_cols)\n",
        "    return phoneme2vec, feature_dim\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "drU2nEkxyVAH"
      },
      "outputs": [],
      "source": [
        "phoneme2vec, FEATURE_DIM = load_phoneme_vectors(\"/content/drive/MyDrive/Sanskrit NLP/sanskrit_phoneme_vectors (1).csv\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cl-_dkJhQmwD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d7c33668-f31d-4066-b9ad-d0d8a54acc6f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total words: 161656\n"
          ]
        }
      ],
      "source": [
        "def load_words(txt_path):\n",
        "    with open(txt_path, encoding=\"utf-8\") as f:\n",
        "        words = [line.strip() for line in f if line.strip()]\n",
        "    return words\n",
        "\n",
        "words = load_words(\"/content/drive/MyDrive/Sanskrit NLP/161656_RV_Terms.txt\") # Changed from .gdoc to .txt\n",
        "print(\"Total words:\", len(words))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zegmwmbED7Hf"
      },
      "outputs": [],
      "source": [
        "def normalize_word(word):\n",
        "    word = unicodedata.normalize(\"NFC\", word)\n",
        "    word = word.replace(\"-\", \"\")\n",
        "    word = word.replace(r\"^\", \"0\")\n",
        "    word = word.replace(r\"t\", \"\")\n",
        "    word = word.replace(r\"r\", \"\")\n",
        "    word = word.replace(r\"e\", \"\")\n",
        " # Assuming r\"^\", \"0\" should replace empty strings, check intent.\n",
        "    return word"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7mBm5v5f2ciX"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "VEDIC_MARKS = {\"‡•í\", \"‡•ë\"}\n",
        "REMOVE_CHARS = {\"‡•ç\", \"‡§Ω\"}\n",
        "NASAL_MAP = {\n",
        "    \"‡§Å\": \"‡§Ç\"   # candrabindu ‚Üí anusvƒÅra\n",
        "}\n",
        "\n",
        "def normalize_sanskrit(word):\n",
        "    # Remove Vedic accent marks\n",
        "    for ch in VEDIC_MARKS:\n",
        "        word = word.replace(ch, \"\")\n",
        "\n",
        "    # Remove virama and avagraha\n",
        "    for ch in REMOVE_CHARS:\n",
        "        word = word.replace(ch, \"\")\n",
        "\n",
        "    # Map nasalization\n",
        "    for src, tgt in NASAL_MAP.items():\n",
        "        word = word.replace(src, tgt)\n",
        "\n",
        "    # Remove digits (Devanagari + Latin)\n",
        "    word = re.sub(r\"[0-9‡•¶-‡•Ø]\", \"\", word)\n",
        "\n",
        "    # Strip whitespace\n",
        "    word = word.strip()\n",
        "\n",
        "    return word\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xfOPIg0ZRtSF"
      },
      "outputs": [],
      "source": [
        "DEVANAGARI_MATRAS = {\n",
        "    \"‡§æ\": \"‡§Ü\", \"‡§ø\": \"‡§á\", \"‡•Ä\": \"‡§à\", \"‡•Å\": \"‡§â\", \"‡•Ç\": \"‡§ä\",\n",
        "    \"‡•É\": \"‡§ã\", \"‡•Ñ\": \"‡•†\", \"‡•¢\": \"‡§å\", \"‡•£\": \"‡•°\",\n",
        "    \"‡•á\": \"‡§è\", \"‡•à\": \"‡§ê\", \"‡•ã\": \"‡§ì\", \"‡•å\": \"‡§î\",\n",
        "}\n",
        "\n",
        "# independent vowels set\n",
        "DEVANAGARI_VOWELS = set(list(\"‡§Ö‡§Ü‡§á‡§à‡§â‡§ä‡§ã‡•†‡§å‡•°‡§è‡§ê‡§ì‡§î\"))\n",
        "\n",
        "# consonants range roughly (‡§ï..‡§π) ‚Äî we'll treat these as consonants\n",
        "# include nukta & other combining marks handled separately\n",
        "DEVANAGARI_CONSONANTS = set(list(\n",
        "    \"‡§ï‡§ñ‡§ó‡§ò‡§ô‡§ö‡§õ‡§ú‡§ù‡§û‡§ü‡§†‡§°‡§¢‡§£‡§§‡§•‡§¶‡§ß‡§®‡§™‡§´‡§¨‡§≠‡§Æ‡§Ø‡§±‡§≤‡§µ‡§∂‡§∑‡§∏‡§π\"\n",
        "))\n",
        "# add retroflex/rule variants if needed (adjust per data)\n",
        "# Halant, nukta, anusvara, visarga\n",
        "HALANT = \"\\u094D\"     # ‡•ç\n",
        "NUKTA = \"\\u093C\"      # ‡§º\n",
        "ANUSVARA = \"‡§Ç\"\n",
        "VISARGA = \"‡§É\"\n",
        "CANDRABINDU = \"‡§Å\"\n",
        "\n",
        "PHONETIC_MODIFIERS = {ANUSVARA, VISARGA, CANDRABINDU}\n",
        "\n",
        "def words_to_phonemes(word):\n",
        "\n",
        "    word = unicodedata.normalize(\"NFC\", word.strip())\n",
        "    phonemes = []\n",
        "    i = 0\n",
        "    chars = list(word)\n",
        "\n",
        "    while i < len(chars):\n",
        "        ch = chars[i]\n",
        "\n",
        "        # independent vowel\n",
        "        if ch in DEVANAGARI_VOWELS:\n",
        "            phonemes.append(ch)\n",
        "            i += 1\n",
        "            continue\n",
        "\n",
        "        # modifier symbols that act like separate phonemes (anusvara/visarga)\n",
        "        if ch in PHONETIC_MODIFIERS:\n",
        "            phonemes.append(ch)\n",
        "            i += 1\n",
        "            continue\n",
        "\n",
        "        # consonant (including possible nukta immediately after)\n",
        "        if ch in DEVANAGARI_CONSONANTS:\n",
        "            base = ch\n",
        "            i += 1\n",
        "            # nukta (rare) e.g. ‡§ï‡§º\n",
        "            if i < len(chars) and chars[i] == NUKTA:\n",
        "                base = base + chars[i]\n",
        "                i += 1\n",
        "\n",
        "            # halant means explicit consonant without inherent vowel\n",
        "            if i < len(chars) and chars[i] == HALANT:\n",
        "                # append base (consonant) only, skip halant\n",
        "                phonemes.append(base)\n",
        "                i += 1\n",
        "                continue\n",
        "\n",
        "            # vowel matra attached? map to independent vowel and append base+vowel\n",
        "            if i < len(chars) and chars[i] in DEVANAGARI_MATRAS:\n",
        "                mat = chars[i]\n",
        "                vowel = DEVANAGARI_MATRAS[mat]\n",
        "                phonemes.append(base)\n",
        "                phonemes.append(vowel)\n",
        "                i += 1\n",
        "                continue\n",
        "\n",
        "\n",
        "            phonemes.append(base)\n",
        "            continue\n",
        "\n",
        "        # standalone matra (shouldn't usually happen), convert to vowel\n",
        "        if ch in DEVANAGARI_MATRAS:\n",
        "            phonemes.append(DEVANAGARI_MATRAS[ch])\n",
        "            i += 1\n",
        "            continue\n",
        "\n",
        "        # otherwise: unknown char, append as-is (fallback)\n",
        "        phonemes.append(ch)\n",
        "        i += 1\n",
        "\n",
        "    return phonemes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q-dggWCUTbKu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "23c7cb3e-893b-4133-e403-288fb35f4174"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['‡§Ö', '‡§ó', '‡§®', '‡§á', '‡§Æ']\n",
            "['‡§ß', '‡§∞', '‡•ç', '‡§Æ']\n",
            "['‡§ï', '‡§∞', '‡•ç', '‡§Æ']\n"
          ]
        }
      ],
      "source": [
        "print(words_to_phonemes(\"‡§Ö‡§ó‡•ç‡§®‡§ø‡§Æ‡•ç\"))\n",
        "\n",
        "print(words_to_phonemes(\"‡§ß‡§∞‡•ç‡§Æ\"))\n",
        "\n",
        "print(words_to_phonemes(\"‡§ï‡§∞‡•ç‡§Æ\"))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "si9S_UFOUGC_"
      },
      "outputs": [],
      "source": [
        "def phonemes_to_features(phonemes, phoneme2vec):\n",
        "    features = []\n",
        "    for p in phonemes:\n",
        "        if p not in phoneme2vec:\n",
        "            return None   # skip word if phoneme missing\n",
        "        features.append(phoneme2vec[p])\n",
        "    return np.stack(features)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u1GF_gIm0eQ3"
      },
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "import numpy as np\n",
        "import json\n",
        "\n",
        "def preprocess_dataset(words, phoneme2vec):\n",
        "    dataset = []\n",
        "    skipped_words = []\n",
        "    skip_reasons = Counter()\n",
        "\n",
        "    for word in words:\n",
        "        original_word = word\n",
        "        word = normalize_sanskrit(normalize_word(word))\n",
        "\n",
        "        # Phonemization\n",
        "        try:\n",
        "            phonemes = words_to_phonemes(word)\n",
        "        except Exception as e:\n",
        "            skip_reasons[\"phonemizer_error\"] += 1\n",
        "            skipped_words.append({\n",
        "                \"word\": original_word,\n",
        "                \"normalized\": word,\n",
        "                \"reason\": \"phonemizer_error\"\n",
        "            })\n",
        "            continue\n",
        "\n",
        "        # Missing phoneme vectors\n",
        "        missing = [p for p in phonemes if p not in phoneme2vec]\n",
        "        if missing:\n",
        "            skip_reasons[f\"missing_phoneme:{missing[0]}\"] += 1\n",
        "            skipped_words.append({\n",
        "                \"word\": original_word,\n",
        "                \"normalized\": word,\n",
        "                \"phonemes\": phonemes,\n",
        "                \"missing_phoneme\": missing[0],\n",
        "                \"reason\": \"missing_phoneme\"\n",
        "            })\n",
        "            continue\n",
        "\n",
        "        # Valid word\n",
        "        features = np.stack([phoneme2vec[p] for p in phonemes])\n",
        "        dataset.append({\n",
        "            \"word\": word,\n",
        "            \"phonemes\": phonemes,\n",
        "            \"features\": features\n",
        "        })\n",
        "\n",
        "    print(\"\\n SKIP SUMMARY \")\n",
        "    print(\"Total skipped:\", len(skipped_words))\n",
        "    for k, v in skip_reasons.most_common(15):\n",
        "        print(f\"{k:25s} : {v}\")\n",
        "\n",
        "    return dataset, skipped_words\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DMOTz6oyULnm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fe226d46-8a07-4d7f-e68e-ddaeac2ae199"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==== SKIP SUMMARY ====\n",
            "Total skipped: 3138\n",
            "missing_phoneme:          : 3137\n",
            "missing_phoneme:m         : 1\n"
          ]
        }
      ],
      "source": [
        "dataset, skipped_words_info = preprocess_dataset(words, phoneme2vec)\n",
        "\n",
        "# Convert NumPy arrays to lists for JSON serialization\n",
        "serializable_dataset = []\n",
        "for item in dataset:\n",
        "    serializable_item = item.copy()\n",
        "    serializable_item[\"features\"] = item[\"features\"].tolist()\n",
        "    serializable_dataset.append(serializable_item)\n",
        "\n",
        "with open(\"sanskrit_metric_learning_dataset.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(serializable_dataset, f, ensure_ascii=False, indent=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jryq6FxuqX3c"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def substitution_cost(v1, v2):\n",
        "    \"\"\"\n",
        "    v1, v2: (feature_dim,) ternary vectors\n",
        "    \"\"\"\n",
        "    return np.mean(np.abs(v1 - v2))\n",
        "\n",
        "def articulatory_distance(seq1, seq2):\n",
        "    \"\"\"\n",
        "    seq1, seq2: List[np.ndarray]  (phoneme feature sequences)\n",
        "    \"\"\"\n",
        "    n, m = len(seq1), len(seq2)\n",
        "\n",
        "    dp = np.zeros((n + 1, m + 1))\n",
        "\n",
        "    for i in range(1, n + 1):\n",
        "        dp[i, 0] = i\n",
        "    for j in range(1, m + 1):\n",
        "        dp[0, j] = j\n",
        "\n",
        "    for i in range(1, n + 1):\n",
        "        for j in range(1, m + 1):\n",
        "            sub = substitution_cost(seq1[i - 1], seq2[j - 1])\n",
        "            dp[i, j] = min(\n",
        "                dp[i - 1, j] + 1,      # deletion\n",
        "                dp[i, j - 1] + 1,      # insertion\n",
        "                dp[i - 1, j - 1] + sub # substitution\n",
        "            )\n",
        "\n",
        "    return dp[n, m]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o0s6isbz_MFH"
      },
      "outputs": [],
      "source": [
        "class PhoneticEncoder(nn.Module):\n",
        "    def __init__(self, feature_dim, hidden_dim=128, emb_dim=64):\n",
        "        super().__init__()\n",
        "\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=feature_dim,\n",
        "            hidden_size=hidden_dim,\n",
        "            batch_first=True\n",
        "        )\n",
        "        self.proj = nn.Linear(hidden_dim, emb_dim)\n",
        "\n",
        "    def forward(self, x, lengths):\n",
        "\n",
        "        packed = nn.utils.rnn.pack_padded_sequence(\n",
        "            x, lengths, batch_first=True, enforce_sorted=False\n",
        "        )\n",
        "        _, (h_n, _) = self.lstm(packed)\n",
        "        return self.proj(h_n[-1])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IXMqoLCoCmNu"
      },
      "outputs": [],
      "source": [
        "def metric_learning_loss(emb_a, emb_b, art_dist):\n",
        "    \"\"\"\n",
        "    emb_a, emb_b: (B, D)\n",
        "    art_dist: (B,)\n",
        "    \"\"\"\n",
        "    emb_dist = torch.norm(emb_a - emb_b, dim=1)\n",
        "    return ((emb_dist - art_dist) ** 2).mean()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g4BOoe_eEgdM"
      },
      "outputs": [],
      "source": [
        "class PhoneticEncoder(nn.Module):\n",
        "    def __init__(self, feature_dim, hidden_dim, emb_dim):\n",
        "        super().__init__()\n",
        "        self.lstm = nn.LSTM(feature_dim, hidden_dim, batch_first=True)\n",
        "        self.proj = nn.Linear(hidden_dim, emb_dim)\n",
        "\n",
        "    def forward(self, x, lengths):\n",
        "        packed = nn.utils.rnn.pack_padded_sequence(\n",
        "            x, lengths.cpu(), batch_first=True, enforce_sorted=False\n",
        "        )\n",
        "        _, (h_n, _) = self.lstm(packed)\n",
        "        emb = self.proj(h_n[-1])\n",
        "        return emb\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yTKAHScsEvEz"
      },
      "outputs": [],
      "source": [
        "def metric_loss(emb1, emb2, art_dist):\n",
        "    emb_dist = torch.norm(emb1 - emb2, dim=1)\n",
        "    return ((emb_dist - art_dist) ** 2).mean()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "awT4kuwxVWWx"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset\n",
        "import random\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JZ65jYwdbB-n",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4c929aaa-6299-4ddc-e1c5-ddc58b309de3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train size: 126814\n",
            "Validation size: 31704\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train_data, val_data = train_test_split(\n",
        "    dataset,\n",
        "    test_size=0.2,\n",
        "    random_state=42,\n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "print(f\"Train size: {len(train_data)}\")\n",
        "print(f\"Validation size: {len(val_data)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eEm6SpatEwzD"
      },
      "outputs": [],
      "source": [
        "class MetricDataset(Dataset):\n",
        "    def __init__(self, data):\n",
        "        self.data = data\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        j = random.randint(0, len(self.data) - 1)\n",
        "        return idx, j\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2LnGQPXREyYA"
      },
      "outputs": [],
      "source": [
        "def make_collate_fn(data):\n",
        "    def collate_fn(batch):\n",
        "        idx_a, idx_b = zip(*batch)\n",
        "\n",
        "        def pad(idxs):\n",
        "            seqs = [data[i][\"features\"] for i in idxs]\n",
        "            lengths = torch.tensor([len(s) for s in seqs])\n",
        "\n",
        "            max_len = max(lengths)\n",
        "            feature_dim = len(seqs[0][0])\n",
        "\n",
        "            padded = torch.zeros(len(seqs), max_len, feature_dim)\n",
        "            for i, s in enumerate(seqs):\n",
        "                padded[i, :len(s)] = torch.tensor(s)\n",
        "\n",
        "            return padded.float(), lengths\n",
        "\n",
        "        xa, la = pad(idx_a)\n",
        "        xb, lb = pad(idx_b)\n",
        "\n",
        "        art_dist = torch.tensor([\n",
        "            articulatory_distance(\n",
        "                data[a][\"features\"],\n",
        "                data[b][\"features\"]\n",
        "            )\n",
        "            for a, b in zip(idx_a, idx_b)\n",
        "        ]).float()\n",
        "\n",
        "        return xa, la, xb, lb, art_dist\n",
        "\n",
        "    return collate_fn\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "889Rhxd6VboT"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "from functools import lru_cache\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E3gjkUXzVdo2"
      },
      "outputs": [],
      "source": [
        "@lru_cache(maxsize=100_000)\n",
        "def cached_art_dist(a_id, b_id):\n",
        "    return articulatory_distance(\n",
        "        dataset[a_id][\"features\"],\n",
        "        dataset[b_id][\"features\"]\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CBbPaneZdLeV"
      },
      "outputs": [],
      "source": [
        "train_loader = DataLoader(\n",
        "    MetricDataset(train_data),\n",
        "    batch_size=32,\n",
        "    shuffle=True,\n",
        "    collate_fn=make_collate_fn(train_data)\n",
        "    )\n",
        "val_loader = DataLoader(\n",
        "    MetricDataset(val_data),\n",
        "    batch_size=32,\n",
        "    shuffle=False,\n",
        "    collate_fn=make_collate_fn(val_data)\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x4ONlAdgVhGv"
      },
      "outputs": [],
      "source": [
        "def train(\n",
        "    model,\n",
        "    train_loader,\n",
        "    val_loader,\n",
        "    epochs=100,\n",
        "    patience=4,\n",
        "    lr=1e-3\n",
        "):\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    model.to(device)\n",
        "\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "    best_val_loss = float(\"inf\")\n",
        "    patience_counter = 0\n",
        "\n",
        "    for epoch in range(1, epochs + 1):\n",
        "\n",
        "        # TRAIN\n",
        "        model.train()\n",
        "        train_loss = 0.0\n",
        "\n",
        "        for xa, la, xb, lb, art_dist in train_loader:\n",
        "            xa, la = xa.to(device), la.to(device)\n",
        "            xb, lb = xb.to(device), lb.to(device)\n",
        "            art_dist = art_dist.to(device)\n",
        "\n",
        "            ea = model(xa, la)\n",
        "            eb = model(xb, lb)\n",
        "\n",
        "            loss = metric_loss(ea, eb, art_dist)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 5.0)\n",
        "            optimizer.step()\n",
        "\n",
        "            train_loss += loss.item()\n",
        "\n",
        "        train_loss /= len(train_loader)\n",
        "\n",
        "        #  VALIDATION\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for xa, la, xb, lb, art_dist in val_loader:\n",
        "                xa, la = xa.to(device), la.to(device)\n",
        "                xb, lb = xb.to(device), lb.to(device)\n",
        "                art_dist = art_dist.to(device)\n",
        "\n",
        "                ea = model(xa, la)\n",
        "                eb = model(xb, lb)\n",
        "\n",
        "                loss = metric_loss(ea, eb, art_dist)\n",
        "                val_loss += loss.item()\n",
        "\n",
        "        val_loss /= len(val_loader)\n",
        "\n",
        "        #  LOGGING\n",
        "        print(\n",
        "            f\"Epoch {epoch:03d} | \"\n",
        "            f\"Train Loss: {train_loss:.4f} | \"\n",
        "            f\"Val Loss: {val_loss:.4f}\"\n",
        "        )\n",
        "\n",
        "        #  EARLY STOPPING\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            patience_counter = 0\n",
        "\n",
        "            torch.save(\n",
        "                model.state_dict(),\n",
        "                \"sanskrit_metric_learning_model.pt\"\n",
        "            )\n",
        "\n",
        "            print(\" Best model saved\")\n",
        "\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "            print(f\"No improvement ({patience_counter}/{patience})\")\n",
        "\n",
        "            if patience_counter >= patience:\n",
        "                print(\"Early stopping triggered (overfitting detected)\")\n",
        "                break\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SQI6kdAKdGlH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3c20b843-164f-4f0f-f6bd-242ec4fef2f3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 001 | Train Loss: 0.0952 | Val Loss: 0.0371\n",
            "   ‚úì Best model saved\n",
            "Epoch 002 | Train Loss: 0.0321 | Val Loss: 0.0263\n",
            "   ‚úì Best model saved\n",
            "Epoch 003 | Train Loss: 0.0255 | Val Loss: 0.0233\n",
            "   ‚úì Best model saved\n",
            "Epoch 004 | Train Loss: 0.0225 | Val Loss: 0.0202\n",
            "   ‚úì Best model saved\n",
            "Epoch 005 | Train Loss: 0.0210 | Val Loss: 0.0244\n",
            "   ‚úó No improvement (1/10)\n",
            "Epoch 006 | Train Loss: 0.0204 | Val Loss: 0.0209\n",
            "   ‚úó No improvement (2/10)\n",
            "Epoch 007 | Train Loss: 0.0196 | Val Loss: 0.0185\n",
            "   ‚úì Best model saved\n",
            "Epoch 008 | Train Loss: 0.0192 | Val Loss: 0.0257\n",
            "   ‚úó No improvement (1/10)\n",
            "Epoch 009 | Train Loss: 0.0186 | Val Loss: 0.0185\n",
            "   ‚úó No improvement (2/10)\n",
            "Epoch 010 | Train Loss: 0.0184 | Val Loss: 0.0173\n",
            "   ‚úì Best model saved\n",
            "Epoch 011 | Train Loss: 0.0179 | Val Loss: 0.0171\n",
            "   ‚úì Best model saved\n",
            "Epoch 012 | Train Loss: 0.0176 | Val Loss: 0.0170\n",
            "   ‚úì Best model saved\n",
            "Epoch 013 | Train Loss: 0.0177 | Val Loss: 0.0179\n",
            "   ‚úó No improvement (1/10)\n",
            "Epoch 014 | Train Loss: 0.0176 | Val Loss: 0.0173\n",
            "   ‚úó No improvement (2/10)\n",
            "Epoch 015 | Train Loss: 0.0174 | Val Loss: 0.0186\n",
            "   ‚úó No improvement (3/10)\n",
            "Epoch 016 | Train Loss: 0.0171 | Val Loss: 0.0168\n",
            "   ‚úì Best model saved\n",
            "Epoch 017 | Train Loss: 0.0173 | Val Loss: 0.0180\n",
            "   ‚úó No improvement (1/10)\n",
            "Epoch 018 | Train Loss: 0.0170 | Val Loss: 0.0189\n",
            "   ‚úó No improvement (2/10)\n",
            "Epoch 019 | Train Loss: 0.0168 | Val Loss: 0.0171\n",
            "   ‚úó No improvement (3/10)\n",
            "Epoch 020 | Train Loss: 0.0168 | Val Loss: 0.0177\n",
            "   ‚úó No improvement (4/10)\n",
            "Epoch 021 | Train Loss: 0.0169 | Val Loss: 0.0163\n",
            "   ‚úì Best model saved\n",
            "Epoch 022 | Train Loss: 0.0168 | Val Loss: 0.0171\n",
            "   ‚úó No improvement (1/10)\n",
            "Epoch 023 | Train Loss: 0.0166 | Val Loss: 0.0164\n",
            "   ‚úó No improvement (2/10)\n",
            "Epoch 024 | Train Loss: 0.0165 | Val Loss: 0.0166\n",
            "   ‚úó No improvement (3/10)\n",
            "Epoch 025 | Train Loss: 0.0165 | Val Loss: 0.0164\n",
            "   ‚úó No improvement (4/10)\n",
            "Epoch 026 | Train Loss: 0.0164 | Val Loss: 0.0165\n",
            "   ‚úó No improvement (5/10)\n",
            "Epoch 027 | Train Loss: 0.0163 | Val Loss: 0.0162\n",
            "   ‚úì Best model saved\n",
            "Epoch 028 | Train Loss: 0.0164 | Val Loss: 0.0156\n",
            "   ‚úì Best model saved\n",
            "Epoch 029 | Train Loss: 0.0163 | Val Loss: 0.0159\n",
            "   ‚úó No improvement (1/10)\n",
            "Epoch 030 | Train Loss: 0.0162 | Val Loss: 0.0170\n",
            "   ‚úó No improvement (2/10)\n",
            "Epoch 031 | Train Loss: 0.0163 | Val Loss: 0.0157\n",
            "   ‚úó No improvement (3/10)\n",
            "Epoch 032 | Train Loss: 0.0162 | Val Loss: 0.0157\n",
            "   ‚úó No improvement (4/10)\n",
            "Epoch 033 | Train Loss: 0.0161 | Val Loss: 0.0162\n",
            "   ‚úó No improvement (5/10)\n",
            "Epoch 034 | Train Loss: 0.0161 | Val Loss: 0.0153\n",
            "   ‚úì Best model saved\n",
            "Epoch 035 | Train Loss: 0.0160 | Val Loss: 0.0160\n",
            "   ‚úó No improvement (1/10)\n",
            "Epoch 036 | Train Loss: 0.0161 | Val Loss: 0.0159\n",
            "   ‚úó No improvement (2/10)\n",
            "Epoch 037 | Train Loss: 0.0160 | Val Loss: 0.0160\n",
            "   ‚úó No improvement (3/10)\n",
            "Epoch 038 | Train Loss: 0.0160 | Val Loss: 0.0156\n",
            "   ‚úó No improvement (4/10)\n",
            "Epoch 039 | Train Loss: 0.0158 | Val Loss: 0.0157\n",
            "   ‚úó No improvement (5/10)\n",
            "Epoch 040 | Train Loss: 0.0159 | Val Loss: 0.0159\n",
            "   ‚úó No improvement (6/10)\n",
            "Epoch 041 | Train Loss: 0.0159 | Val Loss: 0.0157\n",
            "   ‚úó No improvement (7/10)\n",
            "Epoch 042 | Train Loss: 0.0160 | Val Loss: 0.0160\n",
            "   ‚úó No improvement (8/10)\n",
            "Epoch 043 | Train Loss: 0.0159 | Val Loss: 0.0162\n",
            "   ‚úó No improvement (9/10)\n",
            "Epoch 044 | Train Loss: 0.0158 | Val Loss: 0.0152\n",
            "   ‚úì Best model saved\n",
            "Epoch 045 | Train Loss: 0.0157 | Val Loss: 0.0160\n",
            "   ‚úó No improvement (1/10)\n",
            "Epoch 046 | Train Loss: 0.0158 | Val Loss: 0.0156\n",
            "   ‚úó No improvement (2/10)\n",
            "Epoch 047 | Train Loss: 0.0157 | Val Loss: 0.0155\n",
            "   ‚úó No improvement (3/10)\n",
            "Epoch 048 | Train Loss: 0.0159 | Val Loss: 0.0159\n",
            "   ‚úó No improvement (4/10)\n",
            "Epoch 049 | Train Loss: 0.0157 | Val Loss: 0.0156\n",
            "   ‚úó No improvement (5/10)\n",
            "Epoch 050 | Train Loss: 0.0157 | Val Loss: 0.0155\n",
            "   ‚úó No improvement (6/10)\n",
            "Epoch 051 | Train Loss: 0.0158 | Val Loss: 0.0166\n",
            "   ‚úó No improvement (7/10)\n",
            "Epoch 052 | Train Loss: 0.0157 | Val Loss: 0.0152\n",
            "   ‚úì Best model saved\n",
            "Epoch 053 | Train Loss: 0.0158 | Val Loss: 0.0151\n",
            "   ‚úì Best model saved\n",
            "Epoch 054 | Train Loss: 0.0156 | Val Loss: 0.0157\n",
            "   ‚úó No improvement (1/10)\n",
            "Epoch 055 | Train Loss: 0.0156 | Val Loss: 0.0166\n",
            "   ‚úó No improvement (2/10)\n",
            "Epoch 056 | Train Loss: 0.0157 | Val Loss: 0.0155\n",
            "   ‚úó No improvement (3/10)\n",
            "Epoch 057 | Train Loss: 0.0157 | Val Loss: 0.0156\n",
            "   ‚úó No improvement (4/10)\n",
            "Epoch 058 | Train Loss: 0.0155 | Val Loss: 0.0156\n",
            "   ‚úó No improvement (5/10)\n",
            "Epoch 059 | Train Loss: 0.0155 | Val Loss: 0.0156\n",
            "   ‚úó No improvement (6/10)\n",
            "Epoch 060 | Train Loss: 0.0156 | Val Loss: 0.0153\n",
            "   ‚úó No improvement (7/10)\n",
            "Epoch 061 | Train Loss: 0.0154 | Val Loss: 0.0153\n",
            "   ‚úó No improvement (8/10)\n",
            "Epoch 062 | Train Loss: 0.0156 | Val Loss: 0.0151\n",
            "   ‚úó No improvement (9/10)\n",
            "Epoch 063 | Train Loss: 0.0154 | Val Loss: 0.0157\n",
            "   ‚úó No improvement (10/10)\n",
            "üõë Early stopping triggered (overfitting detected)\n"
          ]
        }
      ],
      "source": [
        "model = PhoneticEncoder(FEATURE_DIM, hidden_dim=128, emb_dim=64)\n",
        "train(\n",
        "    model,\n",
        "    train_loader,\n",
        "    val_loader,\n",
        "    epochs=200,\n",
        "    patience=10\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MkJr8rCkieOY"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b1KlQJaqq1Tb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "91107918-8a6f-45a3-8f35-53a6ad72df51"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PhoneticEncoder(\n",
              "  (lstm): LSTM(34, 128, batch_first=True)\n",
              "  (proj): Linear(in_features=128, out_features=64, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from scipy.spatial.distance import cosine, euclidean\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "model = PhoneticEncoder(FEATURE_DIM,\n",
        "        hidden_dim=128,\n",
        "        emb_dim=64)\n",
        "model.load_state_dict(torch.load(\"/content/sanskrit_metric_learning_model.pt\", map_location=device))\n",
        "model.to(device)\n",
        "model.eval()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XPOyiPtgq7Ht"
      },
      "outputs": [],
      "source": [
        "def get_embedding(word):\n",
        "    word = normalize_word(word)\n",
        "    phonemes = words_to_phonemes(word)\n",
        "\n",
        "    features = phonemes_to_features(phonemes, phoneme2vec)\n",
        "    if features is None:\n",
        "        raise ValueError(f\"Unknown phoneme in word: {word}\")\n",
        "\n",
        "    x = torch.tensor(features).unsqueeze(0).float()  # (1, T, F)\n",
        "    lengths = torch.tensor([x.shape[1]])\n",
        "\n",
        "    with torch.no_grad():\n",
        "        emb = model(x.to(device), lengths.to(device))\n",
        "\n",
        "    return emb.squeeze(0).cpu().numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VslvgVCoq9xW"
      },
      "outputs": [],
      "source": [
        "def compare_words(word1, word2):\n",
        "    e1 = get_embedding(word1)\n",
        "    e2 = get_embedding(word2)\n",
        "\n",
        "    cos_dist = cosine(e1, e2)\n",
        "    euc_dist = euclidean(e1, e2)\n",
        "\n",
        "    return {\n",
        "        \"word1\": word1,\n",
        "        \"word2\": word2,\n",
        "        \"cosine_distance\": float(cos_dist),\n",
        "        \"euclidean_distance\": float(euc_dist)\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nT0_uzUDr6cv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4992326a-8724-49d2-b62b-b09412511c85"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‡§Æ‡§æ‡§¨‡§ø - ‡§Æ‡§æ‡§≠‡•Ä‡§≠ | Euclidean distance: 1.6518 | Cosine distance: 0.1964\n",
            "‡§á‡§π‡•à‡§ß‡§ø - ‡§Ø‡§π‡•á‡§ß‡•Ä | Euclidean distance: 1.1449 | Cosine distance: 0.0806\n",
            "‡§á‡§∑‡•Å‡§∞‡§ø - ‡§á‡§∂‡•Ç‡§∞‡•Ä | Euclidean distance: 0.2620 | Cosine distance: 0.0044\n",
            "‡§â‡§¶‡•á‡§£‡•Ä - ‡§â‡§ß‡•á‡§®‡§ø | Euclidean distance: 0.2912 | Cosine distance: 0.0052\n",
            "‡§á‡§Ø‡§Æ‡§®‡•ç - ‡§Ø‡§Æ‡§®‡•ç‡§§ | Euclidean distance: 0.8898 | Cosine distance: 0.1030\n",
            "‡§®‡§µ‡§™‡•ç - ‡§™‡•ç‡§∞‡§æ‡§£ | Euclidean distance: 1.4128 | Cosine distance: 0.2879\n"
          ]
        }
      ],
      "source": [
        "word_pairs = [\n",
        "    (\"‡§Æ‡§æ‡§¨‡§ø\", \"‡§Æ‡§æ‡§≠‡•Ä‡§≠\"),\n",
        "    (\"‡§á‡§π‡•à‡§ß‡§ø\", \"‡§Ø‡§π‡•á‡§ß‡•Ä\"),\n",
        "    (\"‡§á‡§∑‡•Å‡§∞‡§ø\", \"‡§á‡§∂‡•Ç‡§∞‡•Ä\"),\n",
        "    (\"‡§â‡§¶‡•á‡§£‡•Ä\", \"‡§â‡§ß‡•á‡§®‡§ø\"),\n",
        "    (\"‡§á‡§Ø‡§Æ‡§®‡•ç\", \"‡§Ø‡§Æ‡§®‡•ç‡§§\"),\n",
        "    (\"‡§®‡§µ‡§™‡•ç\", \"‡§™‡•ç‡§∞‡§æ‡§£\")\n",
        "]\n",
        "\n",
        "results = []\n",
        "for word1, word2 in word_pairs:\n",
        "    try:\n",
        "        results.append(compare_words(word1, word2))\n",
        "    except ValueError as e:\n",
        "        results.append({\"word1\": word1, \"word2\": word2, \"error\": str(e)})\n",
        "\n",
        "for res in results:\n",
        "    if \"error\" in res:\n",
        "        print(f\"{res['word1']} - {res['word2']} | Error: {res['error']}\")\n",
        "    else:\n",
        "        print(f\"{res['word1']} - {res['word2']} | Euclidean distance: {res['euclidean_distance']:.4f} | Cosine distance: {res['cosine_distance']:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "epochs = 63, patience = 10\n",
        "* ‡§Æ‡§æ‡§¨‡§ø - ‡§Æ‡§æ‡§≠‡•Ä‡§≠ | Euclidean distance: 1.6518 | Cosine distance: 0.1964\n",
        "* ‡§á‡§π‡•à‡§ß‡§ø - ‡§Ø‡§π‡•á‡§ß‡•Ä | Euclidean distance: 1.1449 | Cosine distance: 0.0806\n",
        "* ‡§á‡§∑‡•Å‡§∞‡§ø - ‡§á‡§∂‡•Ç‡§∞‡•Ä | Euclidean distance: 0.2620 | Cosine distance: 0.0044\n",
        "* ‡§â‡§¶‡•á‡§£‡•Ä - ‡§â‡§ß‡•á‡§®‡§ø | Euclidean distance: 0.2912 | Cosine distance: 0.0052\n",
        "* ‡§á‡§Ø‡§Æ‡§®‡•ç - ‡§Ø‡§Æ‡§®‡•ç‡§§ | Euclidean distance: 0.8898 | Cosine distance: 0.1030\n",
        "* ‡§®‡§µ‡§™‡•ç - ‡§™‡•ç‡§∞‡§æ‡§£ | Euclidean distance: 1.4128 | Cosine distance: 0.2879\n"
      ],
      "metadata": {
        "id": "K5hc_eGsX-P-"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4fEqBBOWXwcO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5gX235UYCqf0"
      },
      "outputs": [],
      "source": [
        "def word_to_articulatory_sequence(word):\n",
        "    word = normalize_sanskrit(word)\n",
        "    phonemes = words_to_phonemes(word)\n",
        "\n",
        "    seq = []\n",
        "    for p in phonemes:\n",
        "        if p not in phoneme2vec:\n",
        "            raise ValueError(f\"Unknown phoneme: {p}\")\n",
        "        seq.append(phoneme2vec[p])\n",
        "\n",
        "    return seq\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xglFRKloCst6"
      },
      "outputs": [],
      "source": [
        "def articulatory_word_distance(word1, word2):\n",
        "    seq1 = word_to_articulatory_sequence(word1)\n",
        "    seq2 = word_to_articulatory_sequence(word2)\n",
        "    return articulatory_distance(seq1, seq2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8G_Cxvs9D4nG"
      },
      "outputs": [],
      "source": [
        "def articulatory_distance_for_pairs(word_pairs):\n",
        "    results = []\n",
        "\n",
        "    for w1, w2 in word_pairs:\n",
        "        try:\n",
        "            dist = articulatory_word_distance(w1, w2)\n",
        "            results.append({\n",
        "                \"word1\": w1,\n",
        "                \"word2\": w2,\n",
        "                \"articulatory_distance\": dist\n",
        "            })\n",
        "        except Exception as e:\n",
        "            results.append({\n",
        "                \"word1\": w1,\n",
        "                \"word2\": w2,\n",
        "                \"articulatory_distance\": None,\n",
        "                \"error\": str(e)\n",
        "            })\n",
        "\n",
        "    return results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "th3xsA71Dw27"
      },
      "outputs": [],
      "source": [
        "def print_articulatory_results(results):\n",
        "    for res in results:\n",
        "        if res[\"articulatory_distance\"] is not None:\n",
        "            print(\n",
        "                f\"{res['word1']} - {res['word2']} | \"\n",
        "                f\"Articulatory distance: {res['articulatory_distance']:.4f}\"\n",
        "            )\n",
        "        else:\n",
        "            print(\n",
        "                f\"{res['word1']} - {res['word2']} | \"\n",
        "                f\"FAILED ({res.get('error', 'unknown error')})\"\n",
        "            )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M7p98X5YDzZR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5f43ec37-8ef0-4c67-aee9-db768e95d059"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‡§Æ‡§æ‡§¨‡§ø - ‡§Æ‡§æ‡§≠‡•Ä‡§≠ | Articulatory distance: 1.1176\n",
            "‡§á‡§π‡•à‡§ß‡§ø - ‡§Ø‡§π‡•á‡§ß‡•Ä | Articulatory distance: 0.8824\n",
            "‡§á‡§∑‡•Å‡§∞‡§ø - ‡§á‡§∂‡•Ç‡§∞‡•Ä | Articulatory distance: 0.1765\n",
            "‡§â‡§¶‡•á‡§£‡•Ä - ‡§â‡§ß‡•á‡§®‡§ø | Articulatory distance: 0.1765\n",
            "‡§á‡§Ø‡§Æ‡§®‡•ç - ‡§Ø‡§Æ‡§®‡•ç‡§§ | Articulatory distance: 0.8235\n",
            "‡§®‡§µ‡§™‡•ç - ‡§™‡•ç‡§∞‡§æ‡§£ | Articulatory distance: 1.5294\n"
          ]
        }
      ],
      "source": [
        "word_pairs = [\n",
        "    (\"‡§Æ‡§æ‡§¨‡§ø\", \"‡§Æ‡§æ‡§≠‡•Ä‡§≠\"),\n",
        "    (\"‡§á‡§π‡•à‡§ß‡§ø\", \"‡§Ø‡§π‡•á‡§ß‡•Ä\"),\n",
        "    (\"‡§á‡§∑‡•Å‡§∞‡§ø\", \"‡§á‡§∂‡•Ç‡§∞‡•Ä\"),\n",
        "    (\"‡§â‡§¶‡•á‡§£‡•Ä\", \"‡§â‡§ß‡•á‡§®‡§ø\"),\n",
        "    (\"‡§á‡§Ø‡§Æ‡§®‡•ç\", \"‡§Ø‡§Æ‡§®‡•ç‡§§\"),\n",
        "    (\"‡§®‡§µ‡§™‡•ç\", \"‡§™‡•ç‡§∞‡§æ‡§£\")\n",
        "]\n",
        "\n",
        "results = articulatory_distance_for_pairs(word_pairs)\n",
        "print_articulatory_results(results)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* ‡§Æ‡§æ‡§¨‡§ø - ‡§Æ‡§æ‡§≠‡•Ä‡§≠ | Articulatory distance: 1.1176\n",
        "* ‡§á‡§π‡•à‡§ß‡§ø - ‡§Ø‡§π‡•á‡§ß‡•Ä | Articulatory distance: 0.8824\n",
        "* ‡§á‡§∑‡•Å‡§∞‡§ø - ‡§á‡§∂‡•Ç‡§∞‡•Ä | Articulatory distance: 0.1765\n",
        "* ‡§â‡§¶‡•á‡§£‡•Ä - ‡§â‡§ß‡•á‡§®‡§ø | Articulatory distance: 0.1765\n",
        "* ‡§á‡§Ø‡§Æ‡§®‡•ç - ‡§Ø‡§Æ‡§®‡•ç‡§§ | Articulatory distance: 0.8235\n",
        "* ‡§®‡§µ‡§™‡•ç - ‡§™‡•ç‡§∞‡§æ‡§£ | Articulatory distance: 1.5294"
      ],
      "metadata": {
        "id": "CMZKgmGHYetE"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "91c3ui_aYd4u"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "toc_visible": true,
      "mount_file_id": "1xH1Xp-8p28uvxhERac8hmyH-g0jo1H7T",
      "authorship_tag": "ABX9TyMVWu0K95eOtdLz+JSJQ2Ln",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}